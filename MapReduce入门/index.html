

<!DOCTYPE html>
<html lang="zh-CN">
<head>
    
    <meta name="author" content="CodeInterviews">
    
    <meta name="description" itemprop="description" content="MapReduce快速入门">
    
    

    
    <link rel="alternative" href="atom.xml" title="CodeInterviews" type="application/atom+xml">
    
    
    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MapReduce快速入门 | CodeInterviews</title>

    <!-- Bootstrap -->
    <link rel="stylesheet" href="/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/style.css">

    <!-- Javascript -->
    <script src="/js/jquery-2.1.0.min.js"></script>
    <script src="/js/jquery.backstretch.min.js"></script>
    <script src="/bootstrap/js/bootstrap.min.js"></script>
    <script src="/js/headroom.min.js"></script>
    <script src="/js/jquery.headroom.min.js"></script>
    <script src="/js/common.js"></script>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <meta name="baidu-site-verification" content="Jvgfj1O0oP" />
    <meta name="360-site-verification" content="902348aeb40d156fb32b748c4071c4bd" />
</head>
<body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-inverse" role="banner">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="http://codeinterviews.com" title="CodeInterviews">CodeInterviews</a>
            </div>

            <div role="navigation" class="collapse navbar-collapse bs-navbar-collapse">
                

                <ul class="nav navbar-nav">
                    
                    <li id="nav-index"><a href="/">首页</a></li>
                    
                    <li id="nav-categories"><a href="/categories">分类</a></li>
                    
                    <li id="nav-tags"><a href="/tags">标签</a></li>
                    
                    <li id="nav-reading"><a href="/tags/#LeetCode">LeetCode</a></li>
                    
                    <li id="nav-labs"><a href="/tags/#Java">Java</a></li>
                    
                    <li id="nav-about"><a href="/tags/#Javascript">Javascript</a></li>
                    

                    <!--<li id="nav-github"><a href="https://github.com/hexo2hexo" target="_blank">GitHub</a></li>-->
                    <li id="nav-rss"><a href="/atom.xml" target="_blank">Rss</a></li>
                    <li id="nav-search"><input type="text" id="search" placeholder="search" /></li>
                </ul>
            </div>
        </div>
    </nav>

    <script>
    var bgRoot = "http://7xkwt1.com1.z0.glb.clouddn.com/background-";
    var bgLength = "74";
    var bgRandom = false;
    var bgImage = "http://7xkwt1.com1.z0.glb.clouddn.com/background-64.jpg";

    $(function() {
        // page-id...
        var pageId = "MapReduce入门/";
        pageId = pageId.substr(0, pageId.indexOf("/"));
        if(pageId === "") pageId = "index";

        $("#nav-" + pageId).addClass("active");
    });
    </script>

    <article class="post container">
    <div class="well post-body first-post last-post">
        <h1>MapReduce快速入门</h1>

        <div class="time-info">
            
<span class="article-tags">
    
    Tags: <a href="/tags/Hadoop/">Hadoop</a>&nbsp;<a href="/tags/Mapreduce/">Mapreduce</a>&nbsp;
</span>



<span class="article-categories">
    Category:
    <a class="article-category-link" href="/categories/project/">project</a>
</span>


        </div>
        <div class="time-info">
            发表: <time datetime="2016-06-30T03:57:53.000Z"
                       itemprop="datePublished">2016-06-29</time>
            
            更新: <time datetime="2016-06-30T03:57:53.000Z"
                       itemprop="dateModified">2016-06-29</time>
            
        </div>

        <div class="post-body-inner">
            
                <div id="toc" class="toc-article well">
                    <strong class="toc-title">大纲</strong>
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#八、总结"><span class="toc-number">1.</span> <span class="toc-text">八、总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#九-MultipleOutputs使用"><span class="toc-number">2.</span> <span class="toc-text">九.MultipleOutputs使用</span></a></li></ol>
                </div>
            

            <p>##一、MapReduce介绍<br>MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念&#34;Map（映射）&#34;和&#34;Reduce（归约）&#34;，和他们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。他极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统上。 当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组。</p>
<a id="more"></a>
<p>##二、搭建MapReduce工程<br>使用Eclipse创建一个名为MapReduce1的Java Project，在MapReduce1下创建一个名为lib的文件夹，放入hadoop-core-1.2.1.jar（在主文件夹下的Downloads/hadoop-1.2.1文件夹下），并把它添加到Build Path，最后在src下创建一个com.shiyanlou.mapreduce的包。</p>
<p><img src="http://anything-about-doc.qiniudn.com/mapreduce/1.png" alt=""></p>
<p>##三、MapReduce基本流程<br>MapReduce的基本流程是，框架会使用FileInputFormat读取文件，默认会根据文件大小的进行记录拆分，这里拆分器叫做InputSplitter。通过InputSplitter将文件拆成若干块，后面也就有若干个mapper与之对应。</p>
<p>InputSplitter里面使用RecordReader对文件块的记录进行读取，生成key/value的pair，调用mapper的map函数去处理。</p>
<p>当然这些流程中有些可以定制，比如InputSplitter的算法可以修改，RecordReader也是可以定制。</p>
<p>而且还有一个非常有效的方法，可以避免mapper将过多的数据传递给reducer。</p>
<p>比如有个例子都是1, 其实可以先用一个HashMap对key做分组，有则value加1, 无则添加到HashMap中。</p>
<p>最后将分组统计后的key/value数据通过context.write方法发送给reducer，能够大大提高效率。</p>
<p>##四、编写简单mapper<br>现在想从日志中提取数据，部分日志文件如下：</p>
<pre><code>2014-05-10 13:36:40,140307000287,536dbacc4700aab274729cca,login
2014-05-10 13:37:46,140310000378,536dbae74700aab274729ccb,login
2014-05-10 13:39:20,140310000382,536dbb284700aab274729ccd,login
2014-05-10 13:39:31,140331001080,536dbb864700aab274729ccf,login
2014-05-10 13:39:45,140331001105,536dbba04700aab274729cd4,login
2014-05-10 13:39:45,140328000969,536dbba04700aab274729ce4,login
2014-05-10 13:39:45,140408001251,536dbba04700aab274729cd8,login
2014-05-10 13:39:45,140328000991,536dbba04700aab274729ce9,login
2014-05-10 13:39:45,140324000633,536dbba14700aab274729cf5,login
2014-05-10 13:39:45,140331001077,536dbba04700aab274729cdd,login
2014-05-10 13:39:45,140408001242,536dbba04700aab274729cd7,login
2014-05-10 13:39:45,140327000941,536dbba14700aab274729cf1,login
2014-05-10 13:39:45,140408001265,536dbba04700aab274729ce5,login
2014-05-10 13:39:45,140324000673,536dbba04700aab274729cd3,login
2014-05-10 13:39:45,140331001066,536dbba04700aab274729cd5,login
2014-05-10 13:39:45,140408001292,536dbba14700aab274729cee,login
2014-05-10 13:39:45,140328000966,536dbba14700aab274729cec,login
2014-05-10 13:39:45,140312000501,536dbba04700aab274729ce1,login
2014-05-10 13:39:45,140306000216,536dbba14700aab274729d02,login
2014-05-10 13:39:45,140327000856,536dbba04700aab274729ce2,login
2014-05-10 13:39:46,140328000985,536dbba14700aab274729cf7,login
2014-05-10 13:39:46,140306000245,536dbba14700aab274729d0d,login
2014-05-10 13:39:46,140326000797,536dbba14700aab274729cf6,login
2014-05-10 13:39:46,140328000993,536dbba14700aab274729d12,login
2014-05-10 13:39:46,140331001115,536dbba14700aab274729d10,login
2014-05-10 13:39:46,140325000744,536dbba04700aab274729ce0,login
2014-05-10 13:39:46,140328000982,536dbba14700aab274729d0a,login
2014-05-10 13:39:46,140331001063,536dbba04700aab274729ce3,login
2014-05-10 13:39:46,140331001067,536dbba14700aab274729d1c,login
2014-05-10 13:39:46,140401001157,536dbba04700aab274729ce8,login
2014-05-10 13:39:46,140408001216,536dbba14700aab274729cef,login
2014-05-10 13:39:46,140401001174,536dbba14700aab274729d27,login
2014-05-10 13:39:46,140306000215,536dbba04700aab274729cde,login
2014-05-10 13:39:46,140331001064,536dbba04700aab274729cdc,login
2014-05-10 13:39:46,140326000825,536dbba04700aab274729cd9,login
2014-05-10 13:39:46,140408001294,536dbba14700aab274729d0f,login
</code></pre><p>希望将login前面的设备ID取出来，进行数量的统计，最后得到结果。<br>例如：</p>
<pre><code>536dbba04700aab274729cdc    1
536dbba04700aab274729cdd    91
536dbba04700aab274729cde    152
</code></pre><p>我们可以创建一个LogMapper类，该类负责做数据的Map，前两个模板参数用于KeyIn和ValueIn, 后两个模板参数用于KeyOut和ValueOut，都是代表类型。</p>
<p>假定一个&lt; KeyIn， ValueIn&gt;组成一个pair，输入的很多pair在一个组里面, 这些pair被一定的算法Map之后，会变成很多组pair。</p>
<p>官方文档：<a href="http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/mapreduce/Mapper.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/mapreduce/Mapper.html</a></p>
<pre><code>Maps input key/value pairs to a set of intermediate key/value pairs.
</code></pre><p>注意，这里的Mapper类用的包是mapreduce，以前有一个老的叫mapred。<br>这里介绍了两者的区别：</p>
<p><a href="http://stackoverflow.com/questions/7598422/is-it-better-to-use-the-mapred-or-the-mapreduce-package-to-create-a-hadoop-job" target="_blank" rel="external">http://stackoverflow.com/questions/7598422/is-it-better-to-use-the-mapred-or-the-mapreduce-package-to-create-a-hadoop-job</a></p>
<p>有两个类LongWritable和IntWritable，用于帮助创建可以Long和Int类型的变量。它们能够帮助将Long和Int的值序列化成字节流，因此都有两个关键方法读入和写出：<br><img src="http://anything-about-doc.qiniudn.com/mapreduce%2F1.jpg" alt=""><br>这个和Hadoop内部RPC调用时采用的序列化算法有关。<br>在com.shiyanlou.mapreduce包下新建一个名为LogMapper的类，代码为：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shiyanlou.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable ONE = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span></span><br><span class="line">			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		String[] line = value.toString().split(<span class="string">","</span>);</span><br><span class="line">		<span class="keyword">if</span> (line.length == <span class="number">4</span>) &#123;</span><br><span class="line">			String dId = line[<span class="number">2</span>];</span><br><span class="line">			context.write(<span class="keyword">new</span> Text(dId), ONE);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个Mapper的子类覆盖了map函数，将字符串用,号拆开后，取出第三个元素作为设备ID, 然后作为key写入context对象。<br>这里value设置为1, 因为后面reduce阶段会简单的求和。</p>
<p>Context类文档参考： <a href="http://hadoop.apache.org/docs/r1.1.1/api/org/apache/hadoop/mapreduce/Mapper.Context.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r1.1.1/api/org/apache/hadoop/mapreduce/Mapper.Context.html</a></p>
<p>write方法不是一般概念的hasmap添加key,value，而是生成一个新的pair对象，里面包含了key和value。 如果多个key相同，也会产生多个pair对象，交给reduce阶段处理。</p>
<p>##六、编写简单reducer<br>Reduce就是做加和统计，在com.shiyanlou.mapreduce包下新建一个名为LogReducer的类，代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shiyanlou.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogReducer</span> <span class="keyword">extends</span></span><br><span class="line">		<span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Key key, Iterable&lt;IntWritable&gt; values, Context context)</span></span><br><span class="line">			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">			sum += val.get();</span><br><span class="line">		&#125;</span><br><span class="line">		result.set(sum);</span><br><span class="line">		context.write(key, result);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里框架保证在调用reduce方法之前，相同的key的value已经被放在values中，从而组成一个pair &lt; key, values&gt;，这些pair之间也已经用key做了排序。</p>
<p>参考文档：<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Reducer.html" target="_blank" rel="external">https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Reducer.html</a></p>
<p>迭代遍历values，取出所有的value，都是1, 简单加和。<br>然后结果写入到context中。 注意，这里的context是Reducer包的Context。</p>
<p>最后，在com.shiyanlou.mapreduce包下新建一个名为LogJob的类，将初始环境设置好。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shiyanlou.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf, <span class="string">"sum_did_from_log_file"</span>);</span><br><span class="line">		job.setJarByClass(LogJob.class);</span><br><span class="line"></span><br><span class="line">		job.setMapperClass(LogMapper.class);</span><br><span class="line">		job.setCombinerClass(LogReducer.class);</span><br><span class="line">		job.setReducerClass(LogReducer.class);</span><br><span class="line"></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>##七、MapReduce例子程序运行<br>在Eclipse中将MapReduce1工程打成jar包，取名为MapReduce1.jar，放到主文件夹下。</p>
<p><img src="http://anything-about-doc.qiniudn.com/mapreduce/2.png" alt=""></p>
<p>接下来进入主文件夹的Downloads/hadoop-1.2.1/bin下，右键打开命令行，首先格式化HDFS：</p>
<pre><code>$ ./hadoop namenode -format
</code></pre><p>然后启动Hadoop所有进程：</p>
<pre><code>$ ./start-all.sh
</code></pre><p>gitclone下来源代码，在主文件夹新建一个input文件夹，把源代码中的log文件放进去，再把input文件夹上传到HDFS：</p>
<pre><code>$ ./hadoop dfs -put ~/input input/
</code></pre><p>同时还要确保output目录不存在，该目录会被MapReduce程序创建，用于存放输出结果：</p>
<pre><code>$ ./hadoop dfs -rmr output
</code></pre><p>运行程序，观察输出结果：</p>
<pre><code>$ ./hadoop jar ~/MapReduce1.jar com.shiyanlou.mapreduce.LogJob input output
</code></pre><p>现在将output目录都复制到本地磁盘，查看结果：</p>
<pre><code>$ ./hadoop dfs -get output/ ~/output
</code></pre><p>然后进入本地output目录查看：</p>
<pre><code>$ cd ~/output
$ ll -alh
$ gedit part-r-00000
</code></pre><p>打开part-r-00000文件，可以看到结果如下：</p>
<pre><code>536dbacc4700aab274729cca    91
536dbae74700aab274729ccb    91
536dbb284700aab274729ccd    91
536dbb864700aab274729ccf    91
536dbba04700aab274729cd3    91
536dbba04700aab274729cd4    91
536dbba04700aab274729cd5    91
536dbba04700aab274729cd7    91
536dbba04700aab274729cd8    91
536dbba04700aab274729cd9    1
536dbba04700aab274729cdc    1
536dbba04700aab274729cdd    91
536dbba04700aab274729cde    152
536dbba04700aab274729ce0    87
536dbba04700aab274729ce1    87
536dbba04700aab274729ce2    87
536dbba04700aab274729ce3    87
536dbba04700aab274729ce4    91
536dbba04700aab274729ce5    91
536dbba04700aab274729ce8    152
536dbba04700aab274729ce9    91
536dbba14700aab274729cec    87
536dbba14700aab274729cee    87
536dbba14700aab274729cef    138
536dbba14700aab274729cf1    91
536dbba14700aab274729cf5    91
536dbba14700aab274729cf6    87
536dbba14700aab274729cf7    87
536dbba14700aab274729d02    87
536dbba14700aab274729d0a    87
536dbba14700aab274729d0d    87
536dbba14700aab274729d0f    1
536dbba14700aab274729d10    87
536dbba14700aab274729d12    87
536dbba14700aab274729d1c    152
536dbba14700aab274729d27    152
</code></pre><h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>前面介绍了如何编写一个简单的日志提取程序，读取HDFS input目录下的日志文件，然后提取数据后，最终输出到output目录下。</p>
<p>现在梳理一下主要过程，然后提出新的改进目标。</p>
<ul>
<li>可比较的序列化<br>第一个是序列化，这是各种编程技术中常用的。MapReduce的特别之处在于由于key用来排序，所有它既要支持序列化和反序列化，同时也要支持比较大小的操作。因此通常使用的都是接口WritableComparable&lt; T&gt;，这个接口分别从Writable接口和java.lang.Comparable&lt; T&gt;接口继承。前者负责序列化，实现的就是类似流(stream)的功能，后者负责比较。</li>
<li>MapReduce计算流程<br>这里只是概括的介绍主要步骤：</li>
</ul>
<ol>
<li>通过<strong>InputFormat</strong>读取HDFS目录的日志文件的所有行，进行内容分块。然后每个块都会对应一个mapper</li>
<li>调用每个<strong>Mapper</strong>的map函数， 将内容块的数据按照行变成&lt; key, value&gt;格式，作为参数传递. map函数的代码由程序员自己实现，通常key是数据，value是整数，便于做统计。这样，也就将参数&lt; key, value&gt;改成了另一种符合业务逻辑的&lt; key, value&gt;, 通过Context.write方法写出去，随后会被框架交给Reducer</li>
<li><strong>Partitioner</strong>目前这个程序中没有实现自己的类，只是简单使用了Reducer，后面会增加这部分的说明</li>
<li>框架会根据key进行分组，组成&lt; key, values&gt;对， 调用<strong>Reducer</strong>的reduce函数，函数接受到Mapper传递来的&lt; key, values&gt;后再做统计</li>
<li>输出成什么样的格式文件由<strong>OutputFormat</strong>来控制。<br>注意上面的几个粗体字，就是5大MapReduce组件。每个组件都是我们可以继承的类，然后MapReduce框架通过多态的方式来回调我们的子类实现的方法。</li>
</ol>
<ul>
<li>MapReduce Job的配置<br>有了上面的实现，还需要配置Job，并且在hadoop命令行中提交。<br>配置的话，直接new一个Job类，调用set方法进行相应的设置即可。 Job的父类是JobContext。<br>就在这里可以设置上面的5大组件类，用自己的类来替换。还可以设置Reducer的数量。</li>
</ul>
<h2 id="九-MultipleOutputs使用"><a href="#九-MultipleOutputs使用" class="headerlink" title="九.MultipleOutputs使用"></a>九.MultipleOutputs使用</h2><p>在前面的例子中，输出文件名是默认的：</p>
<pre><code>_logs   part-r-00000    _SUCCESS
</code></pre><p>part-r-00000<br>还有一个_SUCCESS文件标志job运行成功。<br>还有一个目录_logs。</p>
<p>但是实际情况中，我们有时候需要根据情况定制我的输出文件名。</p>
<p>比如我要根据did的值分组，产生不同的输出文件。所有did出现次数在[0, 60)的都输出到a文件中，在[60, 100)的输出到b文件，其他输出到c文件。</p>
<p>这里涉及到的输出类是MultipleOutputs类。下面是介绍如何实现。</p>
<p>复制前面一节的MapReduce1工程，命名为MapReduce2，在LogJob.java文件添加几行代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shiyanlou.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf, <span class="string">"sum_did_from_log_file"</span>);</span><br><span class="line">		job.setJarByClass(LogJob.class);</span><br><span class="line"></span><br><span class="line">		job.setMapperClass(LogMapper.class);</span><br><span class="line">		job.setCombinerClass(LogReducer.class);</span><br><span class="line">		job.setReducerClass(LogReducer.class);</span><br><span class="line"></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//添加MultipleOutputs</span></span><br><span class="line">		MultipleOutputs.addNamedOutput(job, <span class="string">"a"</span>, TextOutputFormat.class,Text.class, IntWritable.class);</span><br><span class="line">		MultipleOutputs.addNamedOutput(job, <span class="string">"b"</span>, TextOutputFormat.class,Text.class, IntWritable.class);</span><br><span class="line">		MultipleOutputs.addNamedOutput(job, <span class="string">"c"</span>, TextOutputFormat.class,Text.class, IntWritable.class);</span><br><span class="line"></span><br><span class="line">		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>MultipleOutputs.addNamedOutput 函数被调用了三次，设置了文件名为a，b和c，最后两个参数分别是output key和output value类型，应该和job.setOutputKeyClass以及job.setOutputValueClass保持一致。<br>最后修改LogReducer类的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shiyanlou.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> MultipleOutputs outputs;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"enter LogReducer:::setup method"</span>);</span><br><span class="line">		outputs = <span class="keyword">new</span> MultipleOutputs(context);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException,</span><br><span class="line">			InterruptedException </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"enter LogReducer:::cleanup method"</span>);</span><br><span class="line">		outputs.close();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span><br><span class="line">			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"enter LogReducer::reduce method"</span>);</span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">			sum += val.get();</span><br><span class="line">		&#125;</span><br><span class="line">		result.set(sum);</span><br><span class="line">		System.out.println(<span class="string">"key: "</span> + key.toString() + <span class="string">" sum: "</span>+ sum);</span><br><span class="line">		<span class="keyword">if</span> ((sum &lt; <span class="number">60</span>) &amp;&amp; (sum &gt;= <span class="number">0</span>)) &#123;</span><br><span class="line">			outputs.write(<span class="string">"a"</span>, key, sum);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (sum &lt; <span class="number">100</span>) &#123;</span><br><span class="line">			outputs.write(<span class="string">"b"</span>, key, sum);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			outputs.write(<span class="string">"c"</span>, key, sum);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据相同key(did)sum的结果大小，写入到不同的文件中。运行后观察一下结果（运行步骤跟前一节一样）：</p>
<pre><code>$ ls
a-m-00000  b-m-00000  c-m-00000  _logs  part-r-00000  _SUCCESS
</code></pre><p>打开a-m-00000文件：</p>
<pre><code>536dbba04700aab274729cd9    1
536dbba04700aab274729cdc    1
536dbba14700aab274729d0f    1
</code></pre><p>打开b-m-00000文件：</p>
<pre><code>536dbacc4700aab274729cca    91
536dbae74700aab274729ccb    91
536dbb284700aab274729ccd    91
536dbb864700aab274729ccf    91
536dbba04700aab274729cd3    91
536dbba04700aab274729cd4    91
536dbba04700aab274729cd5    91
536dbba04700aab274729cd7    91
536dbba04700aab274729cd8    91
536dbba04700aab274729cdd    91
536dbba04700aab274729ce0    87
536dbba04700aab274729ce1    87
536dbba04700aab274729ce2    87
536dbba04700aab274729ce3    87
536dbba04700aab274729ce4    91
536dbba04700aab274729ce5    91
536dbba04700aab274729ce9    91
536dbba14700aab274729cec    87
536dbba14700aab274729cee    87
536dbba14700aab274729cf1    91
536dbba14700aab274729cf5    91
536dbba14700aab274729cf6    87
536dbba14700aab274729cf7    87
536dbba14700aab274729d02    87
536dbba14700aab274729d0a    87
536dbba14700aab274729d0d    87
536dbba14700aab274729d10    87
536dbba14700aab274729d12    87
</code></pre><p>打开c-m-00000文件：</p>
<pre><code>536dbba04700aab274729cde    152
536dbba04700aab274729ce8    152
536dbba14700aab274729cef    138
536dbba14700aab274729d1c    152
536dbba14700aab274729d27    152
</code></pre><p>结果正确，使用MultipleOutputs根据sum值对设备ID进行分组成功了。<br>MapReduce仍然会默认生成part文件，不用理会，都是空文件。</p>


            <div class="post-info">
    <p class="eof">- EOF -</p>
    <p class="copyright">All rights reserved <a href="http://www.codeinterviews.com">@codeinterviews</a>.</p>
    <p class="link">本文链接：<a href="http://codeinterviews.com/MapReduce入门/">http://codeinterviews.com/MapReduce入门/</a></p>
    <div class="share">
        分享本文：
        <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网"></a>
</div>

<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

    </div>
</div>


            
    <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'disqus_username';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



        </div>
    </div>
</article>

    <footer id="footer">
        <div id="bottom-tip">
            CodeInterviews
        </div>
        <small>该博客由 <a href="https://hexo.io/" target="_blank">Hexo</a> 强力驱动，搭载 <a href="https://github.com/gejiawen/hexadillax2" target="_blank">Hexadillax2</a> 主题</small><br />
        <!--<small>如果你访问github速度过慢，可移步本站的备份站点<a href="http://gejiawen.gitcafe.io">gejiawen.gitcafe.io</a></small><br />-->
        <small>&copy; 2016 <a href="http://codeinterviews.com" target="_blank">CodeInterviews</a>&nbsp;<a href="http://www.miitbeian.gov.cn/" target="_blank">皖ICP备16008778号</a></small>
    </footer>
    
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e4dd778a6204eb51e4f25460e37481ad";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>



<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-51347904-1', 'auto');
  ga('send', 'pageview');
</script>


</body>
</html>

